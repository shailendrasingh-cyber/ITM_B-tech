{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f063d523",
   "metadata": {},
   "source": [
    "# Car Price Prediction — Improved Notebook\n",
    "\n",
    "This notebook:\n",
    "- Loads `/mnt/data/quikr_car.csv` (your uploaded dataset).\n",
    "- Performs EDA with extra visualizations.\n",
    "- Preprocesses categorical and numeric features using a sklearn `ColumnTransformer`.\n",
    "- Trains multiple models (Linear Regression, RandomForest, XGBoost if installed).\n",
    "- Compares model performance and saves the final pipeline (encoder + model) to a `.pkl` for web app use.\n",
    "\n",
    "**Files produced by running the notebook:**\n",
    "- `model_pipeline.pkl` — saved sklearn pipeline (preprocessor + model)\n",
    "- `CAR_PRICE_PREDICTION_UPDATED.ipynb` — this notebook (you are opening it)\n",
    "\n",
    "Run all cells to reproduce results. If you share the notebook with others, they must have the dataset at the same path or update the path accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76bebb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# ML imports\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import joblib\n",
    "\n",
    "# Optional: xgboost (may need pip install)\n",
    "try:\n",
    "    from xgboost import XGBRegressor\n",
    "    xgb_available = True\n",
    "except Exception as e:\n",
    "    xgb_available = False\n",
    "    XGBRegressor = None\n",
    "\n",
    "print('xgboost available:', xgb_available)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e570425f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load dataset\n",
    "data_path = Path('/mnt/data/quikr_car.csv')\n",
    "if not data_path.exists():\n",
    "    raise FileNotFoundError(f\"Dataset not found at {data_path}. Please upload or change the path.\")\n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "print('Dataset shape:', df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c30857",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Quick info and missing values\n",
    "display(df.info())\n",
    "display(df.describe(include='all').T)\n",
    "print('\\nMissing values per column:')\n",
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee349b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visualizations\n",
    "# Adjust these columns depending on your dataset's actual column names.\n",
    "# Common columns in quikr car data: 'price', 'brand', 'model', 'year', 'km_driven', 'fuel_type', 'owner_type', 'transmission', 'location'\n",
    "\n",
    "# Try to auto-detect some likely columns\n",
    "cols = df.columns.tolist()\n",
    "print('Columns detected:', cols)\n",
    "\n",
    "price_col = None\n",
    "for c in ['price','Price','selling_price','Selling_Price']:\n",
    "    if c in cols:\n",
    "        price_col = c\n",
    "        break\n",
    "\n",
    "numeric_candidates = df.select_dtypes(include=['int64','float64']).columns.tolist()\n",
    "cat_candidates = df.select_dtypes(include=['object','category']).columns.tolist()\n",
    "\n",
    "print('\\\\nNumeric candidates:', numeric_candidates)\n",
    "print('Categorical candidates:', cat_candidates)\n",
    "\n",
    "# Distribution of price (if present)\n",
    "if price_col:\n",
    "    plt.figure(figsize=(8,5))\n",
    "    sns.histplot(df[price_col].dropna(), bins=50, kde=True)\n",
    "    plt.title('Price distribution')\n",
    "    plt.xlabel(price_col)\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(8,5))\n",
    "    sns.boxplot(x=df[price_col])\n",
    "    plt.title('Price boxplot')\n",
    "    plt.show()\n",
    "else:\n",
    "    print('No obvious price column detected; please set price_col variable to your target column name.')\n",
    "\n",
    "# Correlation heatmap (numeric)\n",
    "if len(numeric_candidates) >= 2:\n",
    "    plt.figure(figsize=(10,8))\n",
    "    corr = df[numeric_candidates].corr()\n",
    "    sns.heatmap(corr, annot=True, fmt='.2f', cmap='coolwarm')\n",
    "    plt.title('Numeric features correlation matrix')\n",
    "    plt.show()\n",
    "else:\n",
    "    print('Not enough numeric columns for correlation heatmap.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3af8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preprocessing + Model training pipeline\n",
    "# You'll need to set the target column name (e.g., 'price'). Change if needed.\n",
    "TARGET = 'price' if 'price' in df.columns else ( 'Price' if 'Price' in df.columns else None )\n",
    "if TARGET is None:\n",
    "    raise ValueError('Could not auto-detect target column. Please set TARGET manually to your target column name.')\n",
    "\n",
    "# Drop rows with missing target\n",
    "df_model = df.dropna(subset=[TARGET]).copy()\n",
    "X = df_model.drop(columns=[TARGET])\n",
    "y = df_model[TARGET].astype(float)\n",
    "\n",
    "# Fill simple missing values for demonstration\n",
    "# Numeric: median, Categorical: 'missing'\n",
    "numeric_features = X.select_dtypes(include=['int64','float64']).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=['object','category']).columns.tolist()\n",
    "\n",
    "print('Numeric features:', numeric_features)\n",
    "print('Categorical features:', categorical_features)\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ], remainder='drop'\n",
    ")\n",
    "\n",
    "# Models to try\n",
    "models = {\n",
    "    'LinearRegression': LinearRegression(),\n",
    "    'RandomForest': RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "}\n",
    "if xgb_available:\n",
    "    models['XGBoost'] = XGBRegressor(random_state=42, n_jobs=-1, verbosity=0)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train each model and collect metrics\n",
    "results = []\n",
    "pipelines = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    pipe = Pipeline(steps=[('preprocessor', preprocessor), ('model', model)])\n",
    "    print(f'\\nTraining {name}...')\n",
    "    pipe.fit(X_train, y_train)\n",
    "    y_pred = pipe.predict(X_test)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    results.append({'model': name, 'r2': r2, 'rmse': rmse, 'mae': mae})\n",
    "    pipelines[name] = pipe\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values('r2', ascending=False).reset_index(drop=True)\n",
    "display(results_df)\n",
    "\n",
    "# Choose best model by R2\n",
    "best_name = results_df.loc[0,'model']\n",
    "best_pipeline = pipelines[best_name]\n",
    "print('\\nBest model:', best_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735af53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the best pipeline (preprocessor + model) to a .pkl for your web app\n",
    "output_path = Path('/mnt/data/model_pipeline.pkl')\n",
    "joblib.dump(best_pipeline, output_path)\n",
    "print('Saved pipeline to', output_path)\n",
    "\n",
    "# Also save the list of categorical columns (useful for web app form generation)\n",
    "meta = {\n",
    "    'numeric_features': numeric_features,\n",
    "    'categorical_features': categorical_features,\n",
    "    'target': TARGET,\n",
    "    'model_name': best_name\n",
    "}\n",
    "joblib.dump(meta, Path('/mnt/data/model_meta.pkl'))\n",
    "print('Saved metadata to /mnt/data/model_meta.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4ccd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Feature importances (for tree-based models)\n",
    "import numpy as np\n",
    "best_model = best_pipeline.named_steps['model']\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    # need to get column names after preprocessing\n",
    "    # get OHE feature names\n",
    "    cat_ohe = best_pipeline.named_steps['preprocessor'].named_transformers_['cat'].named_steps['onehot']\n",
    "    cat_names = []\n",
    "    try:\n",
    "        cat_names = cat_ohe.get_feature_names_out(categorical_features).tolist()\n",
    "    except:\n",
    "        # older sklearn compatibility\n",
    "        cat_names = []\n",
    "        for i, col in enumerate(categorical_features):\n",
    "            # fallback will just use column name with index\n",
    "            cat_names.append(col)\n",
    "    feature_names = numeric_features + cat_names\n",
    "    importances = best_model.feature_importances_\n",
    "    fi = pd.Series(importances, index=feature_names).sort_values(ascending=False).head(30)\n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.barplot(x=fi.values, y=fi.index)\n",
    "    plt.title('Top feature importances')\n",
    "    plt.show()\n",
    "else:\n",
    "    print('Best model has no feature_importances_ attribute.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6181c4",
   "metadata": {},
   "source": [
    "## Streamlit deployment example\n",
    "\n",
    "Below is a minimal Streamlit app snippet that loads `model_pipeline.pkl` and `model_meta.pkl` and provides a simple form for prediction. Save it as `app.py` and run `streamlit run app.py`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfbd999",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# streamlit_app example (save as app.py) - run externally\n",
    "streamlit_code = r\\\"\\\"\\\"\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "MODEL_PATH = Path('model_pipeline.pkl')\n",
    "META_PATH = Path('model_meta.pkl')\n",
    "\n",
    "pipeline = joblib.load(MODEL_PATH)\n",
    "meta = joblib.load(META_PATH)\n",
    "\n",
    "st.title('Car Price Prediction')\n",
    "\n",
    "# Build form\n",
    "with st.form('predict_form'):\n",
    "    inputs = {}\n",
    "    for col in meta['numeric_features']:\n",
    "        inputs[col] = st.number_input(col, value=0.0)\n",
    "    for col in meta['categorical_features']:\n",
    "        inputs[col] = st.text_input(col, value='')\n",
    "    submitted = st.form_submit_button('Predict')\n",
    "    if submitted:\n",
    "        X = pd.DataFrame([inputs])\n",
    "        pred = pipeline.predict(X)[0]\n",
    "        st.success(f'Predicted {meta[\\\"target\\\"]}: {pred}')\n",
    "\\\"\\\"\\\"\n",
    "print(streamlit_code)\n",
    "# Write file for convenience\n",
    "Path('/mnt/data/streamlit_app_example.py').write_text(streamlit_code)\n",
    "print('Saved Streamlit example to /mnt/data/streamlit_app_example.py')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ff8585",
   "metadata": {},
   "source": [
    "### Notes & Next steps\n",
    "\n",
    "- If `xgboost` isn't installed, you can install it with `pip install xgboost` and re-run the notebook.\n",
    "- Tweak preprocessing and feature selection depending on the actual dataset columns.\n",
    "- For production, consider saving a sklearn `ColumnTransformer` + model as a single pipeline (this notebook does that).\n",
    "- If you want, I can also produce a ready-to-deploy Streamlit app that includes better input widgets (dropdowns, validation) and example screenshots."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}

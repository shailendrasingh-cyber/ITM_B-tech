{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e912994-d487-4911-8734-b290b70d2840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:95: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - loss: 303543943168.0000 - mae: 403076.7500 - val_loss: 196823941120.0000 - val_mae: 336985.4375\n",
      "Epoch 2/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 303542829056.0000 - mae: 403075.2500 - val_loss: 196822302720.0000 - val_mae: 336982.9688\n",
      "Epoch 3/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 303539257344.0000 - mae: 403070.7188 - val_loss: 196816732160.0000 - val_mae: 336974.7812\n",
      "Epoch 4/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 303528083456.0000 - mae: 403056.7500 - val_loss: 196800053248.0000 - val_mae: 336950.8125\n",
      "Epoch 5/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 303496036352.0000 - mae: 403018.3438 - val_loss: 196757291008.0000 - val_mae: 336890.4688\n",
      "Epoch 6/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 303425028096.0000 - mae: 402930.6250 - val_loss: 196664393728.0000 - val_mae: 336761.4062\n",
      "Epoch 7/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 303275933696.0000 - mae: 402755.6875 - val_loss: 196491608064.0000 - val_mae: 336522.0938\n",
      "Epoch 8/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 303006220288.0000 - mae: 402443.6562 - val_loss: 196185227264.0000 - val_mae: 336101.6562\n",
      "Epoch 9/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 302583087104.0000 - mae: 401925.0938 - val_loss: 195692494848.0000 - val_mae: 335426.6562\n",
      "Epoch 10/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 301873004544.0000 - mae: 401093.9062 - val_loss: 194951479296.0000 - val_mae: 334411.0000\n",
      "Epoch 11/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 300878594048.0000 - mae: 399895.4688 - val_loss: 193880752128.0000 - val_mae: 332939.2188\n",
      "Epoch 12/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 299439587328.0000 - mae: 398210.7500 - val_loss: 192409747456.0000 - val_mae: 330908.3750\n",
      "Epoch 13/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 297474850816.0000 - mae: 395798.0312 - val_loss: 190400593920.0000 - val_mae: 328122.8750\n",
      "Epoch 14/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 294859079680.0000 - mae: 392763.5000 - val_loss: 187837677568.0000 - val_mae: 324526.5000\n",
      "Epoch 15/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 291461693440.0000 - mae: 388648.2500 - val_loss: 184564039680.0000 - val_mae: 319888.4375\n",
      "Epoch 16/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 287370936320.0000 - mae: 383681.4688 - val_loss: 180538408960.0000 - val_mae: 314098.7812\n",
      "Epoch 17/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 282127794176.0000 - mae: 377147.0312 - val_loss: 175699001344.0000 - val_mae: 306994.0938\n",
      "Epoch 18/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 276183547904.0000 - mae: 369858.1875 - val_loss: 170072014848.0000 - val_mae: 298524.6250\n",
      "Epoch 19/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 268909068288.0000 - mae: 360418.5312 - val_loss: 163433562112.0000 - val_mae: 288229.2500\n",
      "Epoch 20/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 260906549248.0000 - mae: 349681.2812 - val_loss: 155985100800.0000 - val_mae: 276467.9375\n",
      "Epoch 21/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 251348353024.0000 - mae: 336929.2500 - val_loss: 147780026368.0000 - val_mae: 263324.4375\n",
      "Epoch 22/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 241245519872.0000 - mae: 322940.4062 - val_loss: 139116314624.0000 - val_mae: 248796.4219\n",
      "Epoch 23/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 230647201792.0000 - mae: 307188.6875 - val_loss: 129806630912.0000 - val_mae: 233085.3438\n",
      "Epoch 24/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 218896531456.0000 - mae: 290480.5625 - val_loss: 120123113472.0000 - val_mae: 216547.3125\n",
      "Epoch 25/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 206984052736.0000 - mae: 273039.7188 - val_loss: 110889197568.0000 - val_mae: 200385.7656\n",
      "Epoch 26/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 194386935808.0000 - mae: 256616.9844 - val_loss: 102217490432.0000 - val_mae: 185621.4062\n",
      "Epoch 27/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 183773626368.0000 - mae: 242453.7188 - val_loss: 93892681728.0000 - val_mae: 172881.6406\n",
      "Epoch 28/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 172270747648.0000 - mae: 229268.8750 - val_loss: 86749593600.0000 - val_mae: 164730.9219\n",
      "Epoch 29/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 163533062144.0000 - mae: 218220.9062 - val_loss: 80567369728.0000 - val_mae: 159265.8594\n",
      "Epoch 30/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 155724529664.0000 - mae: 211982.0312 - val_loss: 75829174272.0000 - val_mae: 157045.7344\n",
      "Epoch 31/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 147445596160.0000 - mae: 205564.6250 - val_loss: 72142839808.0000 - val_mae: 156976.7812\n",
      "Epoch 32/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 141682573312.0000 - mae: 199771.1094 - val_loss: 69429608448.0000 - val_mae: 157982.7812\n",
      "Epoch 33/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 138094985216.0000 - mae: 200992.0000 - val_loss: 67443552256.0000 - val_mae: 160113.9375\n",
      "Epoch 34/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 134237298688.0000 - mae: 199120.5312 - val_loss: 66005704704.0000 - val_mae: 162952.7344\n",
      "Epoch 35/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 129790509056.0000 - mae: 196313.1094 - val_loss: 65029095424.0000 - val_mae: 164362.8438\n",
      "Epoch 36/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 125934698496.0000 - mae: 195327.2344 - val_loss: 64269508608.0000 - val_mae: 165519.3438\n",
      "Epoch 37/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 122583162880.0000 - mae: 195429.7812 - val_loss: 63602294784.0000 - val_mae: 166037.2500\n",
      "Epoch 38/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 125594738688.0000 - mae: 198985.9844 - val_loss: 62967394304.0000 - val_mae: 166497.6406\n",
      "Epoch 39/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 120133730304.0000 - mae: 191850.3594 - val_loss: 62442545152.0000 - val_mae: 167202.4844\n",
      "Epoch 40/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 121390841856.0000 - mae: 195946.5000 - val_loss: 62055432192.0000 - val_mae: 168615.2344\n",
      "Epoch 41/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 114718146560.0000 - mae: 194731.4375 - val_loss: 61324107776.0000 - val_mae: 167680.7812\n",
      "Epoch 42/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 117622554624.0000 - mae: 193542.8594 - val_loss: 60644962304.0000 - val_mae: 166904.7344\n",
      "Epoch 43/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 115018555392.0000 - mae: 190197.7969 - val_loss: 60030521344.0000 - val_mae: 166241.1719\n",
      "Epoch 44/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 113998462976.0000 - mae: 190862.0156 - val_loss: 59284611072.0000 - val_mae: 164647.8594\n",
      "Epoch 45/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 111587778560.0000 - mae: 187586.8125 - val_loss: 59013664768.0000 - val_mae: 165633.7188\n",
      "Epoch 46/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 113671536640.0000 - mae: 191759.2812 - val_loss: 58294403072.0000 - val_mae: 164114.3750\n",
      "Epoch 47/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 108722470912.0000 - mae: 186664.8906 - val_loss: 57986998272.0000 - val_mae: 164717.9531\n",
      "Epoch 48/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 107236179968.0000 - mae: 185596.0938 - val_loss: 57367486464.0000 - val_mae: 163627.6875\n",
      "Epoch 49/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 105920577536.0000 - mae: 184920.4375 - val_loss: 56784465920.0000 - val_mae: 162696.6250\n",
      "Epoch 50/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 105888292864.0000 - mae: 183658.5156 - val_loss: 56450199552.0000 - val_mae: 162867.8906\n",
      "Epoch 51/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 105962651648.0000 - mae: 187841.5938 - val_loss: 56192233472.0000 - val_mae: 163416.6094\n",
      "Epoch 52/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 103766048768.0000 - mae: 183756.6719 - val_loss: 55501373440.0000 - val_mae: 161762.5312\n",
      "Epoch 53/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 100988821504.0000 - mae: 179342.0938 - val_loss: 54660284416.0000 - val_mae: 159344.9375\n",
      "Epoch 54/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 101279793152.0000 - mae: 181337.5781 - val_loss: 54213292032.0000 - val_mae: 158786.7969\n",
      "Epoch 55/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 100521926656.0000 - mae: 178044.2656 - val_loss: 53573365760.0000 - val_mae: 157163.4844\n",
      "Epoch 56/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 101627379712.0000 - mae: 179839.3594 - val_loss: 53171417088.0000 - val_mae: 157013.0156\n",
      "Epoch 57/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 97766449152.0000 - mae: 178424.7969 - val_loss: 52733542400.0000 - val_mae: 156532.0312\n",
      "Epoch 58/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 99889250304.0000 - mae: 179422.1562 - val_loss: 52780560384.0000 - val_mae: 158243.4219\n",
      "Epoch 59/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 98128011264.0000 - mae: 176851.0625 - val_loss: 52000542720.0000 - val_mae: 156072.5156\n",
      "Epoch 60/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 95745146880.0000 - mae: 174481.1094 - val_loss: 51348201472.0000 - val_mae: 154346.2812\n",
      "Epoch 61/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 96247783424.0000 - mae: 172786.6406 - val_loss: 50813112320.0000 - val_mae: 153074.7656\n",
      "Epoch 62/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 92844843008.0000 - mae: 171475.0781 - val_loss: 50272829440.0000 - val_mae: 151786.4531\n",
      "Epoch 63/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 93902053376.0000 - mae: 171168.0156 - val_loss: 49752543232.0000 - val_mae: 150572.8125\n",
      "Epoch 64/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 93209976832.0000 - mae: 170466.8281 - val_loss: 49166368768.0000 - val_mae: 149077.2812\n",
      "Epoch 65/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 93213179904.0000 - mae: 170558.2969 - val_loss: 48639811584.0000 - val_mae: 147975.4531\n",
      "Epoch 66/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 91376549888.0000 - mae: 167050.8906 - val_loss: 48302157824.0000 - val_mae: 147725.2812\n",
      "Epoch 67/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 91431837696.0000 - mae: 167983.0938 - val_loss: 47860760576.0000 - val_mae: 147065.2344\n",
      "Epoch 68/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 87709171712.0000 - mae: 164264.3594 - val_loss: 47474233344.0000 - val_mae: 146522.0000\n",
      "Epoch 69/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 87883882496.0000 - mae: 164327.9688 - val_loss: 47308173312.0000 - val_mae: 147195.9688\n",
      "Epoch 70/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 88375394304.0000 - mae: 161827.2656 - val_loss: 46736896000.0000 - val_mae: 145635.4219\n",
      "Epoch 71/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 86165200896.0000 - mae: 162766.0000 - val_loss: 46436024320.0000 - val_mae: 145516.3125\n",
      "Epoch 72/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 86380675072.0000 - mae: 165266.4062 - val_loss: 45955534848.0000 - val_mae: 144593.7812\n",
      "Epoch 73/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 84327636992.0000 - mae: 162086.1094 - val_loss: 45132898304.0000 - val_mae: 141831.6250\n",
      "Epoch 74/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 83714973696.0000 - mae: 160410.2031 - val_loss: 44914266112.0000 - val_mae: 142276.5000\n",
      "Epoch 75/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 82964512768.0000 - mae: 158732.8125 - val_loss: 44558053376.0000 - val_mae: 141877.3594\n",
      "Epoch 76/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 81359683584.0000 - mae: 160231.9531 - val_loss: 44192387072.0000 - val_mae: 141505.8125\n",
      "Epoch 77/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 82876375040.0000 - mae: 161801.6719 - val_loss: 43791867904.0000 - val_mae: 140914.6562\n",
      "Epoch 78/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 81595260928.0000 - mae: 158624.6719 - val_loss: 43370426368.0000 - val_mae: 140198.2500\n",
      "Epoch 79/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 78746755072.0000 - mae: 156147.3906 - val_loss: 42963337216.0000 - val_mae: 139674.5938\n",
      "Epoch 80/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 79718187008.0000 - mae: 153743.9531 - val_loss: 42469965824.0000 - val_mae: 138673.0938\n",
      "Epoch 81/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 80894001152.0000 - mae: 156917.2500 - val_loss: 41935167488.0000 - val_mae: 137546.3281\n",
      "Epoch 82/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 75140464640.0000 - mae: 154608.4062 - val_loss: 41414369280.0000 - val_mae: 136350.2656\n",
      "Epoch 83/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 76376162304.0000 - mae: 153603.4375 - val_loss: 41250058240.0000 - val_mae: 136971.5469\n",
      "Epoch 84/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 76311248896.0000 - mae: 154555.4531 - val_loss: 40882081792.0000 - val_mae: 136856.3438\n",
      "Epoch 85/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 73956147200.0000 - mae: 153080.9688 - val_loss: 40577781760.0000 - val_mae: 136945.5312\n",
      "Epoch 86/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 75332968448.0000 - mae: 152488.2656 - val_loss: 39837728768.0000 - val_mae: 134619.0781\n",
      "Epoch 87/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 73113559040.0000 - mae: 150335.8438 - val_loss: 39518760960.0000 - val_mae: 134521.6719\n",
      "Epoch 88/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 68907991040.0000 - mae: 147607.4531 - val_loss: 38767288320.0000 - val_mae: 131872.7500\n",
      "Epoch 89/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 70186098688.0000 - mae: 146808.6250 - val_loss: 38383439872.0000 - val_mae: 131455.4062\n",
      "Epoch 90/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 75291844608.0000 - mae: 152085.6719 - val_loss: 37846675456.0000 - val_mae: 130248.1797\n",
      "Epoch 91/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 68542013440.0000 - mae: 147110.3281 - val_loss: 37518336000.0000 - val_mae: 130073.7422\n",
      "Epoch 92/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 69192531968.0000 - mae: 142610.4375 - val_loss: 37032308736.0000 - val_mae: 129051.0859\n",
      "Epoch 93/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 69739569152.0000 - mae: 142263.3438 - val_loss: 36395290624.0000 - val_mae: 127045.7812\n",
      "Epoch 94/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 64473743360.0000 - mae: 139319.5000 - val_loss: 36078559232.0000 - val_mae: 127071.4219\n",
      "Epoch 95/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 65138847744.0000 - mae: 141762.8438 - val_loss: 35843911680.0000 - val_mae: 127566.4688\n",
      "Epoch 96/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 64989806592.0000 - mae: 139356.7969 - val_loss: 35261693952.0000 - val_mae: 126076.4922\n",
      "Epoch 97/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 65175412736.0000 - mae: 139198.4219 - val_loss: 34691960832.0000 - val_mae: 124657.8594\n",
      "Epoch 98/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 63446233088.0000 - mae: 137597.7500 - val_loss: 34230667264.0000 - val_mae: 123744.9219\n",
      "Epoch 99/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 64233996288.0000 - mae: 141172.3906 - val_loss: 33772767232.0000 - val_mae: 122963.6250\n",
      "Epoch 100/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 62290161664.0000 - mae: 136152.7344 - val_loss: 33557334016.0000 - val_mae: 123706.3281\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 127914008576.0000 - mae: 189372.5938 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss (MSE): 127914008576.00\n",
      "Test MAE: 189372.59\n",
      "✅ Model and Preprocessor saved successfully!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step\n",
      "Predicted Price: ₹419464.72\n"
     ]
    }
   ],
   "source": [
    "# =======================\n",
    "# STEP 1: Import Libraries\n",
    "# =======================\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "import joblib\n",
    "\n",
    "# =======================\n",
    "# STEP 2: Load Dataset\n",
    "# =======================\n",
    "data = pd.read_csv('cleaned car.csv')\n",
    "\n",
    "# Drop unnecessary column\n",
    "data = data.drop('Unnamed: 0', axis=1)\n",
    "\n",
    "# =======================\n",
    "# STEP 3: Define Features & Target\n",
    "# =======================\n",
    "X = data.drop('Price', axis=1)\n",
    "y = data['Price']\n",
    "\n",
    "# =======================\n",
    "# STEP 4: Preprocessing (Encoding + Scaling)\n",
    "# =======================\n",
    "categorical_cols = ['name', 'company', 'fuel_type']\n",
    "numerical_cols = ['year', 'kms_driven']\n",
    "\n",
    "# One-Hot Encode categorical columns & scale numeric\n",
    "ct = ColumnTransformer([\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'), categorical_cols),\n",
    "    ('scaler', StandardScaler(), numerical_cols)\n",
    "], remainder='drop')\n",
    "\n",
    "X_processed = ct.fit_transform(X)\n",
    "\n",
    "# =======================\n",
    "# STEP 5: Train-Test Split\n",
    "# =======================\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# =======================\n",
    "# STEP 6: Build ANN Model\n",
    "# =======================\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_dim=X_train.shape[1]),\n",
    "    Dropout(0.2),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)  # Regression output\n",
    "])\n",
    "\n",
    "# =======================\n",
    "# STEP 7: Compile Model\n",
    "# =======================\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "# =======================\n",
    "# STEP 8: Train Model\n",
    "# =======================\n",
    "history = model.fit(X_train, y_train, validation_split=0.2, epochs=100, batch_size=32, verbose=1)\n",
    "\n",
    "# =======================\n",
    "# STEP 9: Evaluate Model\n",
    "# =======================\n",
    "loss, mae = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss (MSE): {loss:.2f}\")\n",
    "print(f\"Test MAE: {mae:.2f}\")\n",
    "\n",
    "# =======================\n",
    "# STEP 10: Save Model and Preprocessor\n",
    "# =======================\n",
    "model.save('car_price_ann_model12.h5')\n",
    "joblib.dump(ct, 'car_preprocessor12.pkl')\n",
    "print(\"Model and Preprocessor saved successfully!\")\n",
    "\n",
    "# =======================\n",
    "# STEP 11: Optional Test Prediction\n",
    "# =======================\n",
    "sample = pd.DataFrame({\n",
    "    'name': ['Hyundai i10'],\n",
    "    'company': ['Hyundai'],\n",
    "    'year': [2015],\n",
    "    'kms_driven': [35000],\n",
    "    'fuel_type': ['Petrol']\n",
    "})\n",
    "\n",
    "sample_transformed = ct.transform(sample)\n",
    "predicted_price = model.predict(sample_transformed)\n",
    "print(f\"Predicted Price: ₹{predicted_price[0][0]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180c377b-ab20-4db6-800a-d6ee1821d294",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
